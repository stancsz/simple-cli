[
  {
    "id": "1ppmh6r",
    "cron": "@once",
    "prompt": "You are an autonomous agent (supervisor). Your job: create three asynchronous subtasks (using the platform's schedule_task helper) that each write one file to the repository root using the write_files helper. After creating the three subtasks, print EXACTLY one JSON object to stdout with keys {\"fetch_task_id\": \"...\", \"load_task_id\": \"...\", \"dag_task_id\": \"...\"} containing the three spawned task IDs.\n\nRequirements for each child subtask (they must be scheduled with cron='@once'):\n\nSubtask A (fetch_data.py):\n- Use the platform's write_files to create 'fetch_data.py' at repository root with the following Python script content:\n\nimport yfinance as yf\nimport pandas as pd\nimport sys\n\ndef main():\n    try:\n        print('Starting download of AAPL data...')\n        try:\n            data = yf.Ticker('AAPL').history(period='1mo', interval='1d')\n        except Exception:\n            data = yf.download('AAPL', period='1mo', interval='1d')\n\n        if data is None or data.empty:\n            print('Downloaded data is empty', file=sys.stderr)\n            sys.exit(1)\n\n        data = data.reset_index()\n        print(f'Downloaded {len(data)} rows. Writing to aapl.csv...')\n        data.to_csv('aapl.csv', index=False)\n        print('Wrote aapl.csv successfully')\n    except Exception as e:\n        print(f'Failed to download or write data: {e}', file=sys.stderr)\n        sys.exit(1)\n\nif __name__ == '__main__':\n    main()\n\n\n- After write_files completes, the subtask must print EXACTLY the JSON: {\"status\": \"created\", \"file\": \"fetch_data.py\"} on success, or {\"error\": \"...\"} on failure.\n\nSubtask B (load_data.py):\n- Use write_files to create 'load_data.py' at repository root with the following content:\n\nimport pandas as pd\nimport sqlite3\nimport sys\n\ndef main():\n    try:\n        print('Reading aapl.csv...')\n        df = pd.read_csv('aapl.csv', parse_dates=['Date'])\n        if df is None or df.empty:\n            print('aapl.csv is missing or empty', file=sys.stderr)\n            sys.exit(1)\n\n        df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n\n        print('Connecting to SQLite database finance.db...')\n        conn = sqlite3.connect('finance.db')\n        try:\n            print('Writing DataFrame to table stock_prices (if_exists=replace)...')\n            df.to_sql('stock_prices', conn, if_exists='replace', index=False)\n            print('Wrote data to finance.db -> table stock_prices successfully')\n        finally:\n            conn.close()\n    except FileNotFoundError:\n        print('aapl.csv not found in current directory', file=sys.stderr)\n        sys.exit(1)\n    except Exception as e:\n        print(f'Failed to load data into SQLite: {e}', file=sys.stderr)\n        sys.exit(1)\n\nif __name__ == '__main__':\n    main()\n\n- After write_files completes, the subtask must print EXACTLY the JSON: {\"status\": \"created\", \"file\": \"load_data.py\"} on success, or {\"error\": \"...\"} on failure.\n\nSubtask C (airflow_dag.py):\n- Use write_files to create 'airflow_dag.py' at repository root with the following content (valid Airflow 2.x DAG definition):\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow'\n}\n\nwith DAG(\n    dag_id='finance_ingestion',\n    default_args=default_args,\n    start_date=datetime(2025, 1, 1),\n    catchup=False,\n    schedule_interval='@daily'\n) as dag:\n\n    fetch = BashOperator(\n        task_id='fetch',\n        bash_command='python fetch_data.py'\n    )\n\n    load = BashOperator(\n        task_id='load',\n        bash_command='python load_data.py'\n    )\n\n    fetch >> load\n\n- After write_files completes, the subtask must print EXACTLY the JSON: {\"status\": \"created\", \"file\": \"airflow_dag.py\"} on success, or {\"error\": \"...\"} on failure.\n\nSupervisory task instructions (you, the scheduled supervisor):\n1) Immediately spawn three asynchronous tasks by calling the platform's schedule_task helper three times with cron='@once' and the respective prompts (each child prompt should implement its required write_files and final JSON status print). Each schedule_task call will return a task id.\n2) After creating the three child tasks, print EXACTLY one JSON object to stdout with keys:\n   {\"fetch_task_id\": \"<task-id-for-fetch>\", \"load_task_id\": \"<task-id-for-load>\", \"dag_task_id\": \"<task-id-for-dag>\"}\n3) Do not attempt to wait for child tasks to finish; just create them and return their IDs. If any child task creation fails, include its error message as the corresponding value instead of a task id.\n\nImportant: Use the platform's write_files helper inside each child task (not the supervisor) to create the target file, and ensure each child prints its final status JSON as specified.\n\nDescription: Supervisor task that spawns three one-off subtasks which each write one file and return their own status JSON. The supervisor must return the three spawned task IDs as a single JSON object.\n",
    "description": "Supervisor: spawn three one-off subtasks that each write a file and return their async task IDs",
    "lastRun": 1770942208693,
    "enabled": true,
    "failureCount": 0
  }
]