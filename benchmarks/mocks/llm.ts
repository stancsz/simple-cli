import { LLM, LLMResponse, LLMConfig } from "../../src/llm.js";

export class MockLLM extends LLM {
  constructor(config: LLMConfig | LLMConfig[] = []) {
    super(config);
  }

  async generate(
    system: string,
    history: any[],
    signal?: AbortSignal
  ): Promise<LLMResponse> {
    const userMessage = history.find(m => m.role === "user")?.content || "";
    const systemMessage = system || "";

    // 1. Integration Speed Benchmark - Analysis
    if (systemMessage.includes("Analyze the following CLI help text") ||
        systemMessage.includes("Analyze the provided help text") ||
        systemMessage.includes("Analyze an SDK definition") ||
        userMessage.includes("Analyze the following SDK definition")) {
      return this.handleAnalysis(userMessage);
    }

    // 2. Integration Speed Benchmark - Scaffold
    if (systemMessage.includes("Generate a complete MCP server scaffold") ||
        userMessage.includes("Generate an MCP server for") ||
        userMessage.includes("using this analysis")) {
      return this.handleScaffold(userMessage);
    }

    // 3. Token Efficiency Benchmark
    if (userMessage.includes("BENCHMARK_TOKEN_EFFICIENCY")) {
      return this.handleTokenEfficiency(userMessage);
    }

    // Default fallback
    return {
        thought: "Mock thought",
        tool: "none",
        args: {},
        message: "Mock response",
        raw: "{}",
        usage: { promptTokens: 10, completionTokens: 5, totalTokens: 15 }
    };
  }

  private handleAnalysis(prompt: string): LLMResponse {
    let framework = "unknown";
    if (prompt.includes("Roo Code")) framework = "roo_code";
    else if (prompt.includes("SWE-agent")) framework = "swe_agent";
    else if (prompt.includes("Aider")) framework = "aider";

    const analysis = {
        description: `Mock analysis for ${framework}`,
        usage_patterns: [`${framework} run`],
        tools: [
            {
                name: `${framework}_tool`,
                description: `Main tool for ${framework}`,
                args: [{ name: "input", type: "string", description: "Input command" }]
            }
        ]
    };

    return {
        thought: "Analyzing framework...",
        tool: "analysis_result",
        args: analysis,
        raw: JSON.stringify({ tool: "analysis_result", args: analysis }),
        usage: { promptTokens: 500, completionTokens: 100, totalTokens: 600 }
    };
  }

  private handleScaffold(prompt: string): LLMResponse {
      let framework = "unknown";
      if (prompt.includes("Roo Code") || prompt.includes("roo_code")) framework = "roo_code";
      else if (prompt.includes("SWE-agent") || prompt.includes("swe_agent")) framework = "swe_agent";
      else if (prompt.includes("Aider") || prompt.includes("aider")) framework = "aider";
      else {
          try {
             // prompt is JSON string of analysis, try to parse it
             const jsonMatch = prompt.match(/\{[\s\S]*\}/);
             if (jsonMatch) {
                 const analysis = JSON.parse(jsonMatch[0]);
                 if (analysis.description && analysis.description.includes("Mock analysis for")) {
                     const parts = analysis.description.split(" ");
                     framework = parts[parts.length - 1];
                 }
             }
          } catch(e) {}
      }

      // Clean framework name for variable use
      const safeName = framework.replace(/[^a-zA-Z0-9_]/g, "_").toLowerCase();

      const files = {
          "index.ts": `
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import { z } from "zod";

const server = new McpServer({
  name: "${safeName}",
  version: "1.0.0"
});

server.tool(
  "${safeName}_tool",
  { input: z.string() },
  async ({ input }) => {
    return { content: [{ type: "text", text: "Executed ${safeName}: " + input }] };
  }
);

async function main() {
  const transport = new StdioServerTransport();
  await server.connect(transport);
}

main().catch((error) => {
  console.error("Server error:", error);
  process.exit(1);
});
`,
          "README.md": `# ${framework} MCP Server\nGenerated by Simple-CLI Framework Analyzer.`,
          "config.json": JSON.stringify({
              name: safeName,
              version: "1.0.0",
              command: {
                  command: "npx",
                  args: ["tsx", "index.ts"]
              }
          }, null, 2)
      };

      return {
          thought: "Generating scaffold...",
          tool: "scaffold_result",
          args: { files },
          raw: JSON.stringify({ tool: "scaffold_result", args: { files } }),
          usage: { promptTokens: 1000, completionTokens: 500, totalTokens: 1500 }
      };
  }

  private handleTokenEfficiency(prompt: string): LLMResponse {
      const isTraditional = prompt.includes("MODE: TRADITIONAL");

      // Traditional mode includes large context (~8000 tokens)
      // Brain mode includes minimal context (~150 tokens)
      const promptTokens = isTraditional ? 8250 : 145;
      const completionTokens = 120; // Simulated answer length

      return {
          thought: "Processing efficiency request...",
          tool: "none",
          args: {},
          message: "Here is the answer based on context.",
          raw: "Here is the answer based on context.",
          usage: {
              promptTokens,
              completionTokens,
              totalTokens: promptTokens + completionTokens
          }
      };
  }
}
